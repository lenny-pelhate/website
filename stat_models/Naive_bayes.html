<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introduction to Naive Bayes</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        h1, h2, h3 {
            color: #2c3e50;
        }
        h1 {
            text-align: center;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }
        .formula {
            background-color: #f8f9fa;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
        }
        .math-formula {
            text-align: center;
            margin: 15px 0;
            font-size: 18px;
        }
        .pros-cons {
            display: flex;
            justify-content: space-between;
            margin: 20px 0;
        }
        .pros, .cons {
            flex-basis: 48%;
            padding: 15px;
            border-radius: 5px;
        }
        .pros {
            background-color: #e6f7e9;
        }
        .cons {
            background-color: #fce8e8;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
        }
        .example-container {
            background-color: #f0f8ff;
            padding: 15px;
            border-radius: 5px;
            margin: 20px 0;
        }
        code {
            font-family: Consolas, Monaco, 'Andale Mono', monospace;
            background-color: #f5f5f5;
            padding: 2px 4px;
            border-radius: 3px;
        }
        #visualization {
            margin: 20px 0;
            padding: 15px;
            background-color: #fff;
            border: 1px solid #ddd;
            border-radius: 5px;
        }
    </style>
</head>
<body>
    <h1>Introduction to Naive Bayes</h1>
    
    <h2>What is Naive Bayes?</h2>
    <p>
        Naive Bayes is a family of probabilistic classifiers based on applying Bayes' theorem with strong (naive) 
        independence assumptions between the features. Despite its simplicity, Naive Bayes classifiers have been 
        remarkably successful in many real-world situations, especially in text classification and spam filtering.
    </p>
    <p>
        The classifier is "naive" because it assumes that all features are independent of each other, given the class label.
        This assumption rarely holds true in real-world applications, but the algorithm performs surprisingly well despite this.
    </p>
    
    <h2>The Formula</h2>
    <div class="formula">
        <p>Naive Bayes is based on Bayes' theorem:</p>
        <div class="math-formula">
            P(Y|X) = P(X|Y) × P(Y) / P(X)
        </div>
        <p>Where:</p>
        <ul>
            <li>P(Y|X) is the posterior probability of class Y given predictor X</li>
            <li>P(X|Y) is the likelihood - probability of predictor X given class Y</li>
            <li>P(Y) is the prior probability of class Y</li>
            <li>P(X) is the prior probability of predictor X</li>
        </ul>
        
        <p>For classification with multiple features X = (x₁, x₂, ..., xₙ), the naive independence assumption gives us:</p>
        <div class="math-formula">
            P(X|Y) = P(x₁|Y) × P(x₂|Y) × ... × P(xₙ|Y)
        </div>
        
        <p>Therefore, the Naive Bayes classifier can be expressed as:</p>
        <div class="math-formula">
            P(Y|X) ∝ P(Y) × ∏ P(xᵢ|Y)
        </div>
        
        <p>The class prediction is given by:</p>
        <div class="math-formula">
            ŷ = argmax<sub>y</sub> P(Y=y) × ∏<sub>i=1</sub><sup>n</sup> P(x<sub>i</sub>|Y=y)
        </div>
    </div>
    
    <h2>Proof of the Formula</h2>
    <div class="formula">
        <p>Starting with Bayes' theorem:</p>
        <div class="math-formula">
            P(Y|X) = P(X|Y) × P(Y) / P(X)
        </div>
        
        <p>For multiple features X = (x₁, x₂, ..., xₙ):</p>
        <div class="math-formula">
            P(Y|x₁, x₂, ..., xₙ) = P(x₁, x₂, ..., xₙ|Y) × P(Y) / P(x₁, x₂, ..., xₙ)
        </div>
        
        <p>The denominator P(X) is a constant for all classes, so we can write:</p>
        <div class="math-formula">
            P(Y|X) ∝ P(X|Y) × P(Y)
        </div>
        
        <p>Now, using the naive independence assumption:</p>
        <div class="math-formula">
            P(x₁, x₂, ..., xₙ|Y) = P(x₁|Y) × P(x₂|Y) × ... × P(xₙ|Y)
        </div>
        
        <p>This gives us:</p>
        <div class="math-formula">
            P(Y|x₁, x₂, ..., xₙ) ∝ P(Y) × ∏<sub>i=1</sub><sup>n</sup> P(x<sub>i</sub>|Y)
        </div>
        
        <p>The final classification rule chooses the class with the highest probability:</p>
        <div class="math-formula">
            ŷ = argmax<sub>y</sub> P(Y=y) × ∏<sub>i=1</sub><sup>n</sup> P(x<sub>i</sub>|Y=y)
        </div>
        
        <p>In practice, we often use log probabilities to avoid underflow with very small probability values:</p>
        <div class="math-formula">
            ŷ = argmax<sub>y</sub> [log P(Y=y) + ∑<sub>i=1</sub><sup>n</sup> log P(x<sub>i</sub>|Y=y)]
        </div>
    </div>
    
    <h2>Use Cases, Benefits, and Drawbacks</h2>
    
    <h3>Common Use Cases</h3>
    <ul>
        <li>Text classification (document categorization, spam filtering)</li>
        <li>Sentiment analysis</li>
        <li>Recommendation systems</li>
        <li>Medical diagnosis</li>
        <li>Face recognition</li>
    </ul>
    
    <div class="pros-cons">
        <div class="pros">
            <h3>Benefits</h3>
            <ul>
                <li>Simple and easy to implement</li>
                <li>Fast training and prediction</li>
                <li>Works well with small training datasets</li>
                <li>Handles high-dimensional data effectively</li>
                <li>Not sensitive to irrelevant features</li>
                <li>Good for multi-class prediction problems</li>
                <li>Performs well even when the independence assumption doesn't fully hold</li>
            </ul>
        </div>
        
        <div class="cons">
            <h3>Drawbacks</h3>
            <ul>
                <li>The "naive" independence assumption rarely holds in real-world data</li>
                <li>Poor estimator of probabilities (good at classification, not probability estimation)</li>
                <li>Can be outperformed by more sophisticated models</li>
                <li>The "zero frequency" problem (when a categorical variable has a category not observed in training)</li>
                <li>Sensitive to how input data is prepared</li>
                <li>Doesn't capture feature interactions</li>
            </ul>
        </div>
    </div>
    
    <h2>Concrete Example: Email Spam Classification</h2>
    <div class="example-container">
        <p>Let's walk through a simple example of using Naive Bayes for email spam classification.</p>
        
        <h3>Training Data</h3>
        <p>Consider a simple dataset of emails classified as spam or not spam (ham):</p>
        
        <table>
            <tr>
                <th>Email ID</th>
                <th>Contains "money"</th>
                <th>Contains "free"</th>
                <th>Contains "dear"</th>
                <th>Class</th>
            </tr>
            <tr>
                <td>1</td>
                <td>Yes</td>
                <td>Yes</td>
                <td>No</td>
                <td>Spam</td>
            </tr>
            <tr>
                <td>2</td>
                <td>No</td>
                <td>No</td>
                <td>Yes</td>
                <td>Ham</td>
            </tr>
            <tr>
                <td>3</td>
                <td>Yes</td>
                <td>No</td>
                <td>No</td>
                <td>Spam</td>
            </tr>
            <tr>
                <td>4</td>
                <td>No</td>
                <td>Yes</td>
                <td>No</td>
                <td>Spam</td>
            </tr>
            <tr>
                <td>5</td>
                <td>No</td>
                <td>No</td>
                <td>Yes</td>
                <td>Ham</td>
            </tr>
            <tr>
                <td>6</td>
                <td>No</td>
                <td>No</td>
                <td>No</td>
                <td>Ham</td>
            </tr>
        </table>
        
        <h3>Calculate Prior Probabilities</h3>
        <p>P(Spam) = 3/6 = 0.5</p>
        <p>P(Ham) = 3/6 = 0.5</p>
        
        <h3>Calculate Conditional Probabilities</h3>
        <p>For "money":</p>
        <ul>
            <li>P(money=Yes|Spam) = 2/3 = 0.67</li>
            <li>P(money=No|Spam) = 1/3 = 0.33</li>
            <li>P(money=Yes|Ham) = 0/3 = 0</li>
            <li>P(money=No|Ham) = 3/3 = 1</li>
        </ul>
        
        <p>For "free":</p>
        <ul>
            <li>P(free=Yes|Spam) = 2/3 = 0.67</li>
            <li>P(free=No|Spam) = 1/3 = 0.33</li>
            <li>P(free=Yes|Ham) = 0/3 = 0</li>
            <li>P(free=No|Ham) = 3/3 = 1</li>
        </ul>
        
        <p>For "dear":</p>
        <ul>
            <li>P(dear=Yes|Spam) = 0/3 = 0</li>
            <li>P(dear=No|Spam) = 3/3 = 1</li>
            <li>P(dear=Yes|Ham) = 2/3 = 0.67</li>
            <li>P(dear=No|Ham) = 1/3 = 0.33</li>
        </ul>
        
        <h3>Zero-Frequency Problem and Laplace Smoothing</h3>
        <p>
            Notice that P(money=Yes|Ham) = 0, which would make P(Ham|X) = 0 for any email containing "money".
            This is the "zero frequency" problem. To solve it, we apply Laplace smoothing (add 1 to each count):
        </p>
        
        <p>For "money" with Laplace smoothing:</p>
        <ul>
            <li>P(money=Yes|Spam) = (2+1)/(3+2) = 3/5 = 0.6</li>
            <li>P(money=No|Spam) = (1+1)/(3+2) = 2/5 = 0.4</li>
            <li>P(money=Yes|Ham) = (0+1)/(3+2) = 1/5 = 0.2</li>
            <li>P(money=No|Ham) = (3+1)/(3+2) = 4/5 = 0.8</li>
        </ul>
        
        <p>We would apply the same smoothing to the other features.</p>
        
        <h3>Classification Example</h3>
        <p>Now, let's classify a new email that contains "money" and "free" but not "dear":</p>
        
        <p>For Spam:</p>
        <p>P(Spam) × P(money=Yes|Spam) × P(free=Yes|Spam) × P(dear=No|Spam)</p>
        <p>= 0.5 × 0.6 × 0.6 × 0.8 = 0.144</p>
        
        <p>For Ham:</p>
        <p>P(Ham) × P(money=Yes|Ham) × P(free=Yes|Ham) × P(dear=No|Ham)</p>
        <p>= 0.5 × 0.2 × 0.2 × 0.4 = 0.008</p>
        
        <p>Since 0.144 > 0.008, we classify the email as Spam.</p>
    </div>
    
    <h2>Visualization of the Example</h2>
    <div id="visualization">
        <svg width="700" height="380" viewBox="0 0 700 380">
            <!-- Background -->
            <rect x="0" y="0" width="700" height="380" fill="#f7f9fc" rx="5" ry="5" />
            
            <!-- Feature space visualization -->
            <rect x="50" y="50" width="300" height="250" fill="#ffffff" stroke="#cccccc" />
            <text x="200" y="40" text-anchor="middle" font-weight="bold">Feature Space Visualization</text>
            
            <!-- Axis labels -->
            <text x="200" y="315" text-anchor="middle">Contains "free"</text>
            <text x="35" y="175" text-anchor="middle" transform="rotate(-90,35,175)">Contains "money"</text>
            
            <!-- Axis -->
            <line x1="50" y1="300" x2="350" y2="300" stroke="#000" />
            <line x1="50" y1="50" x2="50" y2="300" stroke="#000" />
            
            <!-- Grid lines -->
            <line x1="50" y1="175" x2="350" y2="175" stroke="#ddd" stroke-dasharray="5,5" />
            <line x1="200" y1="50" x2="200" y2="300" stroke="#ddd" stroke-dasharray="5,5" />
            
            <!-- Data points -->
            <circle cx="125" cy="125" r="10" fill="#ff6666" /> <!-- Email 1: Spam -->
            <circle cx="125" cy="250" r="10" fill="#ff6666" /> <!-- Email 3: Spam -->
            <circle cx="275" cy="125" r="10" fill="#ff6666" /> <!-- Email 4: Spam -->
            <circle cx="275" cy="250" r="10" fill="#6666ff" /> <!-- Email 2: Ham -->
            <circle cx="275" cy="250" r="10" fill="#6666ff" /> <!-- Email 5: Ham -->
            <circle cx="275" cy="250" r="10" fill="#6666ff" /> <!-- Email 6: Ham -->
            
            <!-- Legend -->
            <circle cx="400" cy="100" r="10" fill="#ff6666" />
            <text x="420" y="105">Spam</text>
            <circle cx="400" cy="130" r="10" fill="#6666ff" />
            <text x="420" y="135">Ham</text>
            
            <!-- Decision boundary approximation -->
            <path d="M 50 220 C 100 220, 150 180, 350 180" stroke="#333" stroke-width="2" stroke-dasharray="5,5" fill="none" />
            <text x="160" y="210" text-anchor="middle" font-style="italic">Decision Boundary</text>
            
            <!-- Bayes' theorem application -->
            <rect x="50" y="320" width="600" height="50" fill="#e6f7ff" rx="5" ry="5" />
            <text x="350" y="345" text-anchor="middle">
                P(Spam|features) ∝ P(Spam) × P(money|Spam) × P(free|Spam) × P(dear|Spam)
            </text>
            
            <!-- New email classification -->
            <circle cx="125" cy="125" r="15" fill="none" stroke="#00cc00" stroke-width="2" />
            <text x="125" cy="125" text-anchor="middle" font-weight="bold">?</text>
            <text x="580" y="125" font-weight="bold">New Email:</text>
            <text x="580" y="145">Contains "money": Yes</text>
            <text x="580" y="165">Contains "free": Yes</text>
            <text x="580" y="185">Contains "dear": No</text>
            <text x="580" y="215" fill="#ff6666" font-weight="bold">Classified as: Spam</text>
        </svg>
    </div>

    <h2>Summary</h2>
    <p>
        Naive Bayes is a simple yet powerful classification algorithm that applies Bayes' theorem with the "naive" 
        assumption of feature independence. Despite this seemingly limiting assumption, it performs remarkably well
        in practice, especially for text classification tasks. Its computational efficiency, low data requirements,
        and good performance make it a staple algorithm in machine learning, particularly as a baseline or when
        working with high-dimensional data.
    </p>
</body>
</html>